from src.utils import *
from rebasin import RebasinNet
from rebasin.loss import MidLoss

device = "cuda"
from copy import deepcopy

# ----------- helper functions --------------


def lerp(model1, model2, l, temporal_model=None):
    if temporal_model is None:
        temporal_model = deepcopy(model1)
    for p, p1, p2 in zip(temporal_model.parameters(), model1.parameters(), model2.parameters()):
        p.data.copy_((1 - l) * p1.data + l * p2.data)
    for m, m1, m2 in zip(temporal_model.modules(), model1.modules(), model2.modules()):
        if isinstance(m, torch.nn.BatchNorm2d):
            m.running_mean = None
            m.running_var = None
            m.track_running_stats = False
    return temporal_model


def eval_loss_acc(model, dataset, criterion, device):
    model.to(device)
    cumulative_test_loss = 0
    cumulative_test_acc = 0
    total_test = 0
    model.eval()
    param_precision = next(iter(model.parameters())).data.dtype
    for x, y in dataset:
        z, loss = estep(x, y, model, criterion, device, param_precision)
        acc_test = sum([1 if y[i] == z[i] else 0 for i in range(y.shape[0])])
        cumulative_test_loss += loss * x.shape[0]
        cumulative_test_acc += acc_test
        total_test += x.shape[0]
    cumulative_test_loss /= total_test
    cumulative_test_acc /= total_test
    return cumulative_test_loss, cumulative_test_acc


def estep(x, y, model, criterion, device, param_precision):
    z = model(x.to(device).to(param_precision))
    loss_test = criterion(z, y.to(device))
    return z.detach().argmax(1), loss_test.detach().item()


def remove_buffer_flags(model):
    """
    Sets all .is_buffer parameters in the model to None (necessary for sinkhorn-rebasin)
    :param model: the model
    :return: None, modifies the model in-place
    """
    for module in model.modules():
        if "is_buffer" in dict(module.named_parameters()).keys():
            module.is_buffer = None


# --------------------------------------------

model_a = load_model("CIFAR10-VGG11-1x-a")
model_b = load_model("CIFAR10-VGG11-1x-b")
remove_buffer_flags(model_a)
remove_buffer_flags(model_b)

train_aug_loader, train_noaug_loader, test_loader = get_loaders("CIFAR10")

pi_model_b = RebasinNet(model_b, input_shape=(1, 3, 32, 32))

model_a.cuda()
model_b.cuda()
pi_model_b.cuda()

criterion = MidLoss(model_a, criterion=torch.nn.CrossEntropyLoss())
optimizer = torch.optim.AdamW(pi_model_b.p.parameters(), lr=0.1)

for iteration in tqdm(range(20)):
    _ = pi_model_b.train()
    for batch in train_noaug_loader:
        rebased_model = pi_model_b()
        loss_training = criterion(rebased_model, batch[0].float().to(device), batch[1].to(device))
        optimizer.zero_grad()
        loss_training.backward()
        optimizer.step()

pi_model_b.eval()
rebased_model = deepcopy(pi_model_b())

lambdas = torch.linspace(0, 1, 25)
costs_naive = torch.zeros_like(lambdas)
costs_lmc = torch.zeros_like(lambdas)
acc_naive = torch.zeros_like(lambdas)
acc_lmc = torch.zeros_like(lambdas)

for i in tqdm(range(lambdas.shape[0])):
    l = lambdas[i]
    temporal_model = lerp(rebased_model, model_a, l)
    costs_lmc[i], acc_lmc[i] = eval_loss_acc(temporal_model, test_loader, torch.nn.CrossEntropyLoss(), device)
    temporal_model = lerp(model_b, model_a, l)
    costs_naive[i], acc_naive[i] = eval_loss_acc(temporal_model, test_loader, torch.nn.CrossEntropyLoss(), device)

print(acc_naive)
print(acc_lmc)
print(costs_naive)
print(costs_lmc)
