{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021242c",
   "metadata": {},
   "source": [
    "# Train-and-Permute-CIFAR10-VGG11\n",
    "\n",
    "This notebook executes the following:\n",
    "\n",
    "1. We train two normalization-free, 2x width VGG11 networks, calling them model0 and model1.\n",
    "2. We compute a set of permutations which align the neurons of model1 with those of model0. We use a correlation-based method which goes back to https://arxiv.org/abs/1511.07543.\n",
    "3. We evaluate the accuracy of the interpolation between model0 and the neuron-permuted version of model1. It's not very good -- accuracy drops from ~90% at the \"endpoint\" networks to ~73% at the midpoint.\n",
    "4. We investigate the cause of this drop. We find that the later hidden units of the interpolated network have \"collapsed variance\".\n",
    "5. We introduce a method of correcting for this phenomenon: For each unit in the interpolated network, we rescale it such that its statistics are similar to those of the parent/endpoint networks.\n",
    "6. We re-evaluate the interpolated network after this correction has been applied, and find that it gets 86.5% test accuracy. So the gap / \"barrier\" has shrunk from 90-73 = 17% to only ~4%. The barrier in terms of test loss is also below 0.1 after correction.\n",
    "\n",
    "We also note that the idea of resetting the BatchNorm statistics of interpolated networks goes at least back to https://arxiv.org/abs/1803.05407. The method we introduce here can be thought of as a generalization of BatchNorm-reset to networks which don't use BatchNorm!\n",
    "\n",
    "For some comparable numbers we can look at the VGG curve of Figure 4 in Git Re-Basin (https://arxiv.org/abs/2209.04836). The authors use a permutation-only method with LayerNorm-based networks, and report a test loss barrier of ~0.25 for 2x-width VGG-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e400db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:33:23] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:33:23] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:33:23] No GPU found.\n",
      "[codecarbon INFO @ 12:33:23] [setup] CPU Tracking...\n",
      "[codecarbon ERROR @ 12:33:23] Unable to read Intel RAPL files for CPU power, we will use a constant for your CPU power. Please view https://github.com/mlco2/codecarbon/issues/244 for workarounds : [Errno 13] Permission denied: '/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj'\n",
      "[codecarbon INFO @ 12:33:23] Tracking Intel CPU via RAPL interface\n",
      "[codecarbon ERROR @ 12:33:24] Unable to read Intel RAPL files for CPU power, we will use a constant for your CPU power. Please view https://github.com/mlco2/codecarbon/issues/244 for workarounds : [Errno 13] Permission denied: '/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj'\n",
      "[codecarbon INFO @ 12:33:24] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:33:24]   Platform system: Linux-5.19.0-38-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:33:24]   Python version: 3.10.6\n",
      "[codecarbon INFO @ 12:33:24]   Available RAM : 15.004 GB\n",
      "[codecarbon INFO @ 12:33:24]   CPU count: 8\n",
      "[codecarbon INFO @ 12:33:24]   CPU model: AMD Ryzen 7 4700U with Radeon Graphics\n",
      "[codecarbon INFO @ 12:33:24]   GPU count: None\n",
      "[codecarbon INFO @ 12:33:24]   GPU model: None\n",
      "[codecarbon INFO @ 12:33:34] Energy consumed for RAM : 0.003209 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:33:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:33:34] 0.003209 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:33:43] Energy consumed for RAM : 0.000023 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:33:43] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:33:43] 0.000023 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:33:49] Energy consumed for RAM : 0.003232 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:33:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:33:49] 0.003232 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:33:58] Energy consumed for RAM : 0.000047 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:33:58] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:33:58] 0.000047 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:04] Energy consumed for RAM : 0.003256 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:04] 0.003256 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:13] Energy consumed for RAM : 0.000070 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:13] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:13] 0.000070 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:19] Energy consumed for RAM : 0.003279 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:19] 0.003279 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:28] Energy consumed for RAM : 0.000094 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:28] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:28] 0.000094 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:34] Energy consumed for RAM : 0.003303 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:34] 0.003303 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:43] Energy consumed for RAM : 0.000117 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:43] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:43] 0.000117 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:49] Energy consumed for RAM : 0.003326 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:49] 0.003326 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:34:58] Energy consumed for RAM : 0.000141 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:34:58] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:34:58] 0.000141 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:04] Energy consumed for RAM : 0.003349 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:04] 0.003349 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:13] Energy consumed for RAM : 0.000164 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:13] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:13] 0.000164 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:19] Energy consumed for RAM : 0.003373 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:19] 0.003373 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:28] Energy consumed for RAM : 0.000188 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:28] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:28] 0.000188 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:34] Energy consumed for RAM : 0.003396 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:34] 0.003396 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:43] Energy consumed for RAM : 0.000211 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:43] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:43] 0.000211 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:49] Energy consumed for RAM : 0.003420 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:49] 0.003420 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:35:58] Energy consumed for RAM : 0.000234 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:35:58] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:35:58] 0.000234 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:36:04] Energy consumed for RAM : 0.003443 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:36:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:36:04] 0.003443 kWh of electricity used since the begining.\n"
     ]
    }
   ],
   "source": [
    "tracker = EmissionsTracker()\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbe2090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmissionsData(timestamp='2023-05-02T12:39:44', project_name='codecarbon', run_id='b230a49d-7af7-4785-ab45-243868fc9a42', duration=376.7150297164917, emissions=0.0001771687574945463, emissions_rate=0.0004702991479471366, cpu_power=0.0, gpu_power=0.0, ram_power=5.626379013061523, cpu_energy=0, gpu_energy=0, ram_energy=0.0005886005232376954, energy_consumed=0.0005886005232376954, country_name='Germany', country_iso_code='DEU', region='free and hanseatic city of hamburg', cloud_provider='', cloud_region='', os='Linux-5.19.0-38-generic-x86_64-with-glibc2.35', python_version='3.10.6', cpu_count=8, cpu_model='AMD Ryzen 7 4700U with Radeon Graphics', gpu_count=None, gpu_model=None, longitude=9.9946, latitude=53.5544, ram_total_size=15.003677368164062, tracking_mode='machine', on_cloud='N')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:40:04] Energy consumed for RAM : 0.003818 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:40:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:40:04] 0.003818 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:40:19] Energy consumed for RAM : 0.003842 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:40:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:40:19] 0.003842 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:40:34] Energy consumed for RAM : 0.003865 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:40:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:40:34] 0.003865 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:40:49] Energy consumed for RAM : 0.003888 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:40:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:40:49] 0.003888 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:41:04] Energy consumed for RAM : 0.003912 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:41:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:41:04] 0.003912 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:41:19] Energy consumed for RAM : 0.003935 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:41:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:41:19] 0.003935 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:41:34] Energy consumed for RAM : 0.003959 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:41:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:41:34] 0.003959 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:41:49] Energy consumed for RAM : 0.003982 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:41:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:41:49] 0.003982 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:42:04] Energy consumed for RAM : 0.004006 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:42:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:42:04] 0.004006 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:42:19] Energy consumed for RAM : 0.004029 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:42:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:42:19] 0.004029 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:42:34] Energy consumed for RAM : 0.004052 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:42:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:42:34] 0.004052 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:42:49] Energy consumed for RAM : 0.004076 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:42:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:42:49] 0.004076 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:43:04] Energy consumed for RAM : 0.004099 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:43:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:43:04] 0.004099 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:43:19] Energy consumed for RAM : 0.004123 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:43:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:43:19] 0.004123 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:43:34] Energy consumed for RAM : 0.004146 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:43:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:43:34] 0.004146 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:43:49] Energy consumed for RAM : 0.004170 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:43:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:43:49] 0.004170 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:04] Energy consumed for RAM : 0.004193 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:44:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:04] 0.004193 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:19] Energy consumed for RAM : 0.004217 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:44:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:19] 0.004217 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:34] Energy consumed for RAM : 0.004240 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:44:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:34] 0.004240 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:49] Energy consumed for RAM : 0.004263 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:44:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:49] 0.004263 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:04] Energy consumed for RAM : 0.004287 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:45:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:04] 0.004287 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:19] Energy consumed for RAM : 0.004310 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:45:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:19] 0.004310 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:34] Energy consumed for RAM : 0.004334 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:45:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:34] 0.004334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:49] Energy consumed for RAM : 0.004357 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:45:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:49] 0.004357 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:04] Energy consumed for RAM : 0.004381 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:46:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:04] 0.004381 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:19] Energy consumed for RAM : 0.004404 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:46:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:19] 0.004404 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:34] Energy consumed for RAM : 0.004428 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:46:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:34] 0.004428 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:49] Energy consumed for RAM : 0.004451 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:46:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:49] 0.004451 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:04] Energy consumed for RAM : 0.004474 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:47:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:04] 0.004474 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:19] Energy consumed for RAM : 0.004498 kWh. RAM Power : 5.626379013061523 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:47:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:19] 0.004498 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:34] Energy consumed for RAM : 0.004521 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:47:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:34] 0.004521 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:49] Energy consumed for RAM : 0.004545 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:47:49] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:49] 0.004545 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:04] Energy consumed for RAM : 0.004568 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:48:04] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:04] 0.004568 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:19] Energy consumed for RAM : 0.004592 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:48:19] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:19] 0.004592 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:34] Energy consumed for RAM : 0.004615 kWh. RAM Power : 5.626379013061523 W\n",
      "[codecarbon INFO @ 12:48:34] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:34] 0.004615 kWh of electricity used since the begining.\n"
     ]
    }
   ],
   "source": [
    "tracker.final_emissions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a directory to save VGG checkpoints\n",
    "os.makedirs('./vgg', exist_ok=True)\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), 'vgg/%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('vgg/%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabbc157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## CIFAR-10 dataloaders -- we use FFCV because it's fast\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_aug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=500,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/cifar_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation functions\n",
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b6c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1, bn=False):\n",
    "        super(VGG, self).__init__()\n",
    "        self.vgg_name = vgg_name\n",
    "        self.bn = bn\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                if self.bn:\n",
    "                    layers.append(nn.BatchNorm2d(self.w*x))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35050c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(w=1):\n",
    "    model = VGG('VGG11', w=w).cuda()\n",
    "    optimizer = SGD(model.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a972049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:48<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9016\n"
     ]
    }
   ],
   "source": [
    "w = 2\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_v1b' % w)\n",
    "\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_v2b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cacbb",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, norm=True, loader=train_aug_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    if norm:\n",
    "        corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "        return corr\n",
    "    else:\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn):\n",
    "    pre_weights = [\n",
    "        conv.weight,\n",
    "    ]\n",
    "    if conv.bias is not None:\n",
    "        pre_weights.append(conv.bias)\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([\n",
    "            bn.weight,\n",
    "            bn.bias,\n",
    "            bn.running_mean,\n",
    "            bn.running_var,\n",
    "        ])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0db278e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9045, 9016)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = VGG('VGG11', w=w).cuda()\n",
    "model1 = VGG('VGG11', w=w).cuda()\n",
    "load_model(model0, 'vgg11x%d_v1b' % w)\n",
    "load_model(model1, 'vgg11x%d_v2b' % w)\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb660be",
   "metadata": {},
   "source": [
    "## Permuting neurons in model1 to match model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a5f5fa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 98.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 91.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 66.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 55.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 45.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 38.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 41.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    layer = feats1[i]\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # get permutation and permute output of conv and maybe bn\n",
    "        if isinstance(feats1[i+1], nn.BatchNorm2d):\n",
    "            assert isinstance(feats1[i+2], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+3), subnet(model1, i+3))\n",
    "            permute_output(perm_map, feats1[i], feats1[i+1])\n",
    "        else:\n",
    "            assert isinstance(feats1[i+1], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "            permute_output(perm_map, feats1[i], None)\n",
    "        # look for succeeding layer to permute input\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(feats1[j], nn.Conv2d):\n",
    "                next_layer = feats1[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = model1.classifier\n",
    "        permute_input(perm_map, next_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b4d80",
   "metadata": {},
   "source": [
    "### Save permuted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3112ee07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9015\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11x%d_v2b_perm1b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1):\n",
    "    sd0 = torch.load('vgg/%s.pt' % key0)\n",
    "    sd1 = torch.load('vgg/%s.pt' % key1)\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b83991",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.59, 0.815, 73.12, 0.904\n"
     ]
    }
   ],
   "source": [
    "model_a = VGG('VGG11', w=w).cuda()\n",
    "mix_weights(model_a, 0.5, 'vgg11x%d_v1b' % w, 'vgg11x%d_v2b_perm1b' % w)\n",
    "print(full_eval(model_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06da3f",
   "metadata": {},
   "source": [
    "## Diagnosing the problem: measuring unitwise variances\n",
    "\n",
    "The interpolated network got 70% train accuracy -- not very good, much worse than the endpoints which get over 90%. Why does this happen?\n",
    "\n",
    "(We find that) in later layers of the interpolated network, the variance of hidden units has \"collapsed\", relative to the variance of the same hidden units in the original/parent/endpoint networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "860f4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, _ = next(iter(train_aug_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "871893bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# returns (vv0, vva, vv1)\n",
    "# where vv0 is a list of layerwise variances (as defined in paper)\n",
    "# for model0 (alpha=0.0)\n",
    "# and vva is the same for the midpoint (alpha=0.5)\n",
    "# and vv1 for model1 (alpha=1.0)\n",
    "def get_layerwise_vars(k0, k1):\n",
    "    vvs = []\n",
    "    for alpha in [0, 0.5, 1]:\n",
    "        mix_weights(model_a, alpha, k0, k1)\n",
    "        vv = []\n",
    "        for i in [1, 4, 7, 9, 12, 14, 17, 19]:\n",
    "#         for i in [2, 5, 8, 10, 13, 15, 18, 20]:\n",
    "            subnet = model_a.features[:i]\n",
    "            with torch.no_grad(), autocast():\n",
    "                out = subnet(inputs)\n",
    "            out = out.permute(1, 0, 2, 3).reshape(out.size(1), -1)\n",
    "            avg_var = out.var(1).mean()\n",
    "            vv.append(avg_var.item())\n",
    "        vvs.append(np.array(vv))\n",
    "    vv0, vva, vv1 = vvs\n",
    "    return vv0, vva, vv1\n",
    "\n",
    "# returns a list of ratios between the variance of\n",
    "# the weight-interpolation midpoint and the averaged variances\n",
    "# of the two endpoints.\n",
    "def get_layerwise_ratios(k0, k1):\n",
    "    vv0, vva, vv1 = get_layerwise_vars(k0, k1)\n",
    "    vv00 = (vv0+vv1)/2\n",
    "    rr = vva/vv0\n",
    "    return rr\n",
    "\n",
    "k0 = 'vgg11x%d_v1b' % w\n",
    "k1 = 'vgg11x%d_v2b' % w\n",
    "rr1 = get_layerwise_ratios(k0, k1)\n",
    "\n",
    "k0 = 'vgg11x%d_v1b' % w\n",
    "k1 = 'vgg11x%d_v2b_perm1b' % w\n",
    "rr2 = get_layerwise_ratios(k0, k1)\n",
    "\n",
    "plt.plot(rr1, label='Without neuron matching')\n",
    "plt.plot(rr2, label='With neuron matching')\n",
    "# plt.yscale('log')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Layer index')\n",
    "plt.title('VGG11 layerwise variance ratios')\n",
    "plt.ylabel('$\\\\dfrac{\\\\sigma_{0.5}}{(\\\\sigma_0 + \\\\sigma_1)/2}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89396da3",
   "metadata": {},
   "source": [
    "## Solving the problem: Activation Renormalization\n",
    "\n",
    "We introduce a rescaling of each neuron's preactivation in the interpolated network, such that the statistics of each neuron are set to be similar to those of the endpoint networks. Practically, this is accomplished by the temporary introduction of PyTorch nn.BatchNorm2d layers, which are then \"fused\" back into the preceding convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetConv(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        self.h = h = conv.out_channels\n",
    "        self.conv = conv\n",
    "        self.bn = nn.BatchNorm2d(h)\n",
    "        self.rescale = False\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_var):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + 1e-5).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.rescale:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            self.bn(x)\n",
    "        return x\n",
    "\n",
    "def make_tracked_net(net, w):\n",
    "    net1 = VGG('VGG11', w=w).cuda()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    feats1 = net1.features\n",
    "    for i, layer in enumerate(feats1):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            feats1[i] = ResetConv(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "def fuse_conv_bn(conv, bn):\n",
    "    fused = torch.nn.Conv2d(\n",
    "        conv.in_channels,\n",
    "        conv.out_channels,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        bias=True,\n",
    "    )\n",
    "\n",
    "    # setting weights\n",
    "    w_conv = conv.weight.clone()\n",
    "    bn_std = (bn.eps + bn.running_var).sqrt()\n",
    "    gamma = bn.weight / bn_std\n",
    "    fused.weight.data = (w_conv * gamma.reshape(-1, 1, 1, 1))\n",
    "\n",
    "    # setting bias\n",
    "    b_conv = conv.bias if conv.bias is not None else torch.zeros_like(bn.bias)\n",
    "    beta = bn.bias + gamma * (-bn.running_mean + b_conv)\n",
    "    fused.bias.data = beta\n",
    "    \n",
    "    return fused\n",
    "\n",
    "def fuse_tracked_net(net, w):\n",
    "    net1 = VGG('VGG11', w=w).cuda()\n",
    "    feats1 = net1.features\n",
    "    for i, layer in enumerate(net.features):\n",
    "        if isinstance(layer, ResetConv):\n",
    "            conv = fuse_conv_bn(layer.conv, layer.bn)\n",
    "            feats1[i].load_state_dict(conv.state_dict())\n",
    "    net1.classifier.load_state_dict(net.classifier.state_dict())\n",
    "    return net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b8f5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc, train_loss, test_acc, test_loss...\n",
      "(α=0.0) 100.00, 0.002, 90.45, 0.449\n",
      "(α=0.5) 77.59, 0.815, 73.12, 0.904\n",
      "(α=1.0) 99.98, 0.002, 90.15, 0.446\n"
     ]
    }
   ],
   "source": [
    "model0 = VGG('VGG11', w).cuda()\n",
    "model_a = VGG('VGG11', w).cuda()\n",
    "model1 = VGG('VGG11', w).cuda()\n",
    "\n",
    "k0 = 'vgg11x%d_v1b' % w\n",
    "k1 = 'vgg11x%d_v2b_perm1b' % w\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model_a, 0.5, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "print('train_acc, train_loss, test_acc, test_loss...')\n",
    "print('(α=0.0)', full_eval(model0))\n",
    "print('(α=0.5)', full_eval(model_a))\n",
    "print('(α=1.0)', full_eval(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f95d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the statistics of every hidden unit in the endpoint networks\n",
    "## this is done practically using PyTorch BatchNorm2d layers.\n",
    "wrap0 = make_tracked_net(model0, w=w)\n",
    "wrap1 = make_tracked_net(model1, w=w)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)\n",
    "\n",
    "wrap_a = make_tracked_net(model_a, w=w)\n",
    "## set the goal mean/std in added bns of interpolated network, and turn batch renormalization on\n",
    "for m0, m_a, m1 in zip(wrap0.modules(), wrap_a.modules(), wrap1.modules()):\n",
    "    if not isinstance(m0, ResetConv):\n",
    "        continue\n",
    "    # get goal statistics -- interpolate the mean and std of parent networks\n",
    "    mu0 = m0.bn.running_mean\n",
    "    mu1 = m1.bn.running_mean\n",
    "    goal_mean = (mu0 + mu1)/2\n",
    "    var0 = m0.bn.running_var\n",
    "    var1 = m1.bn.running_var\n",
    "    goal_var = ((var0.sqrt() + var1.sqrt())/2).square()\n",
    "    # set these in the interpolated bn controller\n",
    "    m_a.set_stats(goal_mean, goal_var)\n",
    "    # turn rescaling on\n",
    "    m_a.rescale = True\n",
    "    \n",
    "# reset the tracked mean/var and fuse rescalings back into conv layers \n",
    "reset_bn_stats(wrap_a)\n",
    "# fuse the rescaling+shift coefficients back into conv layers\n",
    "model_b = fuse_tracked_net(wrap_a, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8a00564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0.5 corrected) 94.42, 0.167, 86.56, 0.533\n"
     ]
    }
   ],
   "source": [
    "print('(α=0.5 corrected)', full_eval(model_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f9fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1693a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
