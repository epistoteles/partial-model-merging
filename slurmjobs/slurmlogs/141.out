-------------------SBATCH STATS-------------------
Total CPU cores:      6
Visible CPU cores:    3
Visible GPUs:         NVIDIA GeForce RTX 3060
--------------------------------------------------
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_165514-a67a102q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-star-230
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/a67a102q
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:13
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-1.0x-a
train_acc    1.0
train_loss   0.00021066540563576078
test_acc     0.991
test_loss    0.028725409228354693
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▆▆▇▇▇▇▇▇███████████████████████████████
wandb:      test_loss █▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.991
wandb:      test_loss 0.02873
wandb: train_accuracy 0.99995
wandb:     train_loss 0.00055
wandb: 
wandb: 🚀 View run royal-star-230 at: https://wandb.ai/epistoteles/partial-model-merging/runs/a67a102q
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_165514-a67a102q/logs
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_170159-k10em2a2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-water-232
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/k10em2a2
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:15
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-1.0x-b
train_acc    1.0
train_loss   0.0002047684503243848
test_acc     0.9919
test_loss    0.02896839389577508
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▆▇▇▇▇▇▇▇██████████████████████████████
wandb:      test_loss █▄▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9919
wandb:      test_loss 0.02897
wandb: train_accuracy 0.99997
wandb:     train_loss 0.00049
wandb: 
wandb: 🚀 View run playful-water-232 at: https://wandb.ai/epistoteles/partial-model-merging/runs/k10em2a2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_170159-k10em2a2/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_170843-m8ou5rlm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sun-234
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/m8ou5rlm
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:18
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-1.0x-a
train_acc    1.0
train_loss   8.019643555599033e-05
test_acc     0.9921
test_loss    0.027973635587841272
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇█▇▇███▇████████████████████████
wandb:      test_loss █▅▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9921
wandb:      test_loss 0.02797
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00024
wandb: 
wandb: 🚀 View run rural-sun-234 at: https://wandb.ai/epistoteles/partial-model-merging/runs/m8ou5rlm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_170843-m8ou5rlm/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_171533-mod8qy8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-dust-236
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/mod8qy8l
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:16
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-1.0x-b
train_acc    1.0
train_loss   7.496810652204051e-05
test_acc     0.992
test_loss    0.027934000175446273
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇█▇█████████████████████████████
wandb:      test_loss █▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.02793
wandb: train_accuracy 1.0
wandb:     train_loss 0.0002
wandb: 
wandb: 🚀 View run amber-dust-236 at: https://wandb.ai/epistoteles/partial-model-merging/runs/mod8qy8l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_171533-mod8qy8l/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_172217-g7sw2prb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-wood-238
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/g7sw2prb
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:20
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-1.0x-a
train_acc    1.0
train_loss   4.346421190651502e-05
test_acc     0.9922
test_loss    0.029137137345969677
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▆▆▇▇▇▇▇▇█▇████████████████████████████
wandb:      test_loss █▆▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9922
wandb:      test_loss 0.02914
wandb: train_accuracy 1.0
wandb:     train_loss 0.00016
wandb: 
wandb: 🚀 View run sage-wood-238 at: https://wandb.ai/epistoteles/partial-model-merging/runs/g7sw2prb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_172217-g7sw2prb/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_172906-afydra22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-lion-240
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/afydra22
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:18
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-1.0x-b
train_acc    1.0
train_loss   4.309030385532727e-05
test_acc     0.9923
test_loss    0.030391064286231995
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▆▆▆▇▇▇▇▇▇▇▇▇██████████████████████████
wandb:      test_loss █▇▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9923
wandb:      test_loss 0.03039
wandb: train_accuracy 1.0
wandb:     train_loss 0.00014
wandb: 
wandb: 🚀 View run playful-lion-240 at: https://wandb.ai/epistoteles/partial-model-merging/runs/afydra22
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_172906-afydra22/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_173553-9snw54qw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-dawn-242
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/9snw54qw
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:18
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-1.0x-a
train_acc    1.0
train_loss   3.665632637724533e-05
test_acc     0.992
test_loss    0.03294161595404148
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▆▇▇▇▇▇▇▇██▇█████████████████████████
wandb:      test_loss █▅▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.03294
wandb: train_accuracy 1.0
wandb:     train_loss 0.0001
wandb: 
wandb: 🚀 View run vibrant-dawn-242 at: https://wandb.ai/epistoteles/partial-model-merging/runs/9snw54qw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_173553-9snw54qw/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_174247-9n2z6fe7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-monkey-244
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/9n2z6fe7
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:21
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-1.0x-b
train_acc    1.0
train_loss   3.241762707754484e-05
test_acc     0.9922
test_loss    0.036375352554023264
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇██▇█▇██▇▇▇▇████████████████████
wandb:      test_loss █▅▃▂▂▂▁▂▁▁▁▂▁▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9922
wandb:      test_loss 0.03638
wandb: train_accuracy 1.0
wandb:     train_loss 0.00011
wandb: 
wandb: 🚀 View run clear-monkey-244 at: https://wandb.ai/epistoteles/partial-model-merging/runs/9n2z6fe7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_174247-9n2z6fe7/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_174939-lsdgdhb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-morning-246
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/lsdgdhb1
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:25
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-1.0x-a
train_acc    1.0
train_loss   3.135632874394408e-05
test_acc     0.9923
test_loss    0.032331970147788526
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▂▁▆▆▆▇▇▇▇█▇▇▇███████████████████████████
wandb:      test_loss ▇█▃▂▃▂▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9923
wandb:      test_loss 0.03233
wandb: train_accuracy 0.99993
wandb:     train_loss 0.00022
wandb: 
wandb: 🚀 View run glorious-morning-246 at: https://wandb.ai/epistoteles/partial-model-merging/runs/lsdgdhb1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_174939-lsdgdhb1/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_175634-shzva86x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-vortex-248
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/shzva86x
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:21
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-1.0x-b
train_acc    1.0
train_loss   2.4190965761287467e-05
test_acc     0.9918
test_loss    0.03821280188858509
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▆▆▇▇▇▇▇▇▇█▇▇██████████████████████████
wandb:      test_loss █▄▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9918
wandb:      test_loss 0.03821
wandb: train_accuracy 1.0
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run vague-vortex-248 at: https://wandb.ai/epistoteles/partial-model-merging/runs/shzva86x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_175634-shzva86x/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_180325-31svfuqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-yogurt-250
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/31svfuqu
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:25
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-1.0x-a
train_acc    1.0
train_loss   3.667746000246552e-05
test_acc     0.9909
test_loss    0.04183432795107365
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▇▆▇▇▇▇█▇▇▇███████████████████████████
wandb:      test_loss █▆▄▂▃▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9909
wandb:      test_loss 0.04183
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00014
wandb: 
wandb: 🚀 View run likely-yogurt-250 at: https://wandb.ai/epistoteles/partial-model-merging/runs/31svfuqu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_180325-31svfuqu/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_181019-74ag0wr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-thunder-252
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/74ag0wr8
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:26
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-1.0x-b
train_acc    1.0
train_loss   3.069881521848098e-05
test_acc     0.9909
test_loss    0.03603995591402054
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▇▇▇▇▇██▇██████▇██████████████████████
wandb:      test_loss █▅▄▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9909
wandb:      test_loss 0.03604
wandb: train_accuracy 1.0
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run brisk-thunder-252 at: https://wandb.ai/epistoteles/partial-model-merging/runs/74ag0wr8
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_181019-74ag0wr8/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_181716-urutmbud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-valley-254
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/urutmbud
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:23
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-1.0x-a
train_acc    1.0
train_loss   3.44500223036448e-05
test_acc     0.9919
test_loss    0.03590540597215295
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇▇▇▇██▇▇█▇█████████████████████
wandb:      test_loss █▄▃▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9919
wandb:      test_loss 0.03591
wandb: train_accuracy 1.0
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run silver-valley-254 at: https://wandb.ai/epistoteles/partial-model-merging/runs/urutmbud
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_181716-urutmbud/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=1.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_182410-1pbqjkzl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-galaxy-256
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/1pbqjkzl
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:31
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-1.0x-b
train_acc    1.0
train_loss   3.186700838947824e-05
test_acc     0.9916
test_loss    0.037625431269407275
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇▇█▇██▇███▇████████████████████
wandb:      test_loss █▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9916
wandb:      test_loss 0.03763
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run dazzling-galaxy-256 at: https://wandb.ai/epistoteles/partial-model-merging/runs/1pbqjkzl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_182410-1pbqjkzl/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=1.0, epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_183119-daw8dwod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-mountain-258
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/daw8dwod
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:26
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-1.0x-a
train_acc    1.0
train_loss   3.760670845167624e-05
test_acc     0.9913
test_loss    0.04612620379775763
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇█████▇████████████████████████
wandb:      test_loss █▅▄▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9913
wandb:      test_loss 0.04613
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00022
wandb: 
wandb: 🚀 View run distinctive-mountain-258 at: https://wandb.ai/epistoteles/partial-model-merging/runs/daw8dwod
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_183119-daw8dwod/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=1.0, epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_183814-3yyrx3ds
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-monkey-260
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/3yyrx3ds
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:27
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-1.0x-b
train_acc    1.0
train_loss   3.071479158431127e-05
test_acc     0.9915
test_loss    0.04141012225300074
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▆▆▇▇▇▇▇▇█▇▇███████▇███████████████████
wandb:      test_loss █▄▃▂▂▁▂▂▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.04141
wandb: train_accuracy 1.0
wandb:     train_loss 9e-05
wandb: 
wandb: 🚀 View run light-monkey-260 at: https://wandb.ai/epistoteles/partial-model-merging/runs/3yyrx3ds
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_183814-3yyrx3ds/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 260, in check_network_status
    self._loop_check_status(
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 216, in _loop_check_status
    local_handle = request()
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_184513-xyigq9fd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-yogurt-262
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/xyigq9fd
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:16
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-2.0x-a
train_acc    1.0
train_loss   0.00014344505586147231
test_acc     0.9913
test_loss    0.027053391374647618
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▆▆▆▇▇▇▇████████████████████████████████
wandb:      test_loss █▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9913
wandb:      test_loss 0.02705
wandb: train_accuracy 1.0
wandb:     train_loss 0.00031
wandb: 
wandb: 🚀 View run still-yogurt-262 at: https://wandb.ai/epistoteles/partial-model-merging/runs/xyigq9fd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_184513-xyigq9fd/logs
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_185159-rovfx1qh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-violet-264
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/rovfx1qh
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:22
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-2.0x-b
train_acc    0.9999833333333333
train_loss   0.0001510211738301829
test_acc     0.992
test_loss    0.028236436285078527
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▆▆▇▇▇█▇███████████████████████████████
wandb:      test_loss █▃▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.02824
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00039
wandb: 
wandb: 🚀 View run valiant-violet-264 at: https://wandb.ai/epistoteles/partial-model-merging/runs/rovfx1qh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_185159-rovfx1qh/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_185850-sry89n7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-darkness-265
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/sry89n7l
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:26
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-2.0x-a
train_acc    1.0
train_loss   5.21456165794613e-05
test_acc     0.9922
test_loss    0.02580470982939005
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▆▇▇▇▇▇███▇█▇█████████████████████████
wandb:      test_loss █▆▄▃▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9922
wandb:      test_loss 0.0258
wandb: train_accuracy 1.0
wandb:     train_loss 0.00014
wandb: 
wandb: 🚀 View run pleasant-darkness-265 at: https://wandb.ai/epistoteles/partial-model-merging/runs/sry89n7l
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_185850-sry89n7l/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_190546-p0ti9vcl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-donkey-267
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/p0ti9vcl
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:22
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-2.0x-b
train_acc    1.0
train_loss   4.8703328896711655e-05
test_acc     0.9924
test_loss    0.02949702739715576
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▇▇▇▇▇▇▇▇▇█▇██████████████████████████
wandb:      test_loss █▅▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9924
wandb:      test_loss 0.0295
wandb: train_accuracy 1.0
wandb:     train_loss 0.00014
wandb: 
wandb: 🚀 View run peachy-donkey-267 at: https://wandb.ai/epistoteles/partial-model-merging/runs/p0ti9vcl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_190546-p0ti9vcl/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_191237-sexk6kj9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-smoke-269
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/sexk6kj9
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:24
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-2.0x-a
train_acc    1.0
train_loss   2.8809985966897026e-05
test_acc     0.9915
test_loss    0.029816009942442177
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▆▇▇▇▇█▇▇▇▇█▇█████████████████████████
wandb:      test_loss █▆▄▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.02982
wandb: train_accuracy 1.0
wandb:     train_loss 0.0001
wandb: 
wandb: 🚀 View run mild-smoke-269 at: https://wandb.ai/epistoteles/partial-model-merging/runs/sexk6kj9
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_191237-sexk6kj9/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_191932-3hgwj5er
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-cosmos-271
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/3hgwj5er
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:28
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-2.0x-b
train_acc    1.0
train_loss   3.589020157050982e-05
test_acc     0.9929
test_loss    0.029783117491751908
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████████
wandb:      test_loss █▅▃▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9929
wandb:      test_loss 0.02978
wandb: train_accuracy 1.0
wandb:     train_loss 0.0001
wandb: 
wandb: 🚀 View run fresh-cosmos-271 at: https://wandb.ai/epistoteles/partial-model-merging/runs/3hgwj5er
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_191932-3hgwj5er/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_192632-by5pyv1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-meadow-273
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/by5pyv1b
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:28
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-2.0x-a
train_acc    1.0
train_loss   1.993958532390631e-05
test_acc     0.9925
test_loss    0.033684789389371875
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇█▇▇██▇█▇███████████████████████
wandb:      test_loss █▅▃▃▃▂▂▂▂▁▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9925
wandb:      test_loss 0.03368
wandb: train_accuracy 1.0
wandb:     train_loss 9e-05
wandb: 
wandb: 🚀 View run faithful-meadow-273 at: https://wandb.ai/epistoteles/partial-model-merging/runs/by5pyv1b
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_192632-by5pyv1b/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_193340-b0g9287v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-energy-275
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/b0g9287v
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:34
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-2.0x-b
train_acc    1.0
train_loss   1.9672962541032272e-05
test_acc     0.9927
test_loss    0.03370436104014516
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)wandb: | 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▆▆▆▇▇▇▇█▇█▇▇████████████████████████
wandb:      test_loss █▄▄▃▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9927
wandb:      test_loss 0.0337
wandb: train_accuracy 0.99997
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run frosty-energy-275 at: https://wandb.ai/epistoteles/partial-model-merging/runs/b0g9287v
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_193340-b0g9287v/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_194043-817yk1yn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-cosmos-277
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/817yk1yn
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:33
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-2.0x-a
train_acc    1.0
train_loss   2.2653887306963345e-05
test_acc     0.9922
test_loss    0.034014136902987956
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: \ 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▅▆▇▇▇▇▇▇▆▇▇▇▇▇▇███████████████████████
wandb:      test_loss █▇▄▂▂▂▂▂▂▂▃▁▁▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9922
wandb:      test_loss 0.03401
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run lemon-cosmos-277 at: https://wandb.ai/epistoteles/partial-model-merging/runs/817yk1yn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_194043-817yk1yn/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_194755-ujrw2ojc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-snowflake-279
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ujrw2ojc
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:36
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-2.0x-b
train_acc    1.0
train_loss   1.8303659999219235e-05
test_acc     0.9926
test_loss    0.03402864262461662
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▅▆▇▇▇▇▇▇▇█▇██▇████████████████████████
wandb:      test_loss ██▄▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9926
wandb:      test_loss 0.03403
wandb: train_accuracy 1.0
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run floral-snowflake-279 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ujrw2ojc
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_194755-ujrw2ojc/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_195500-vvg0yp3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-cosmos-280
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/vvg0yp3d
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:36
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-2.0x-a
train_acc    1.0
train_loss   1.8077603382001447e-05
test_acc     0.9916
test_loss    0.03900688961148262
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▅▇▇▇▇▇█▇▇▇▇▇█████████████████████████
wandb:      test_loss █▅▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9916
wandb:      test_loss 0.03901
wandb: train_accuracy 1.0
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run magic-cosmos-280 at: https://wandb.ai/epistoteles/partial-model-merging/runs/vvg0yp3d
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_195500-vvg0yp3d/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_200206-vawrn4p0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-gorge-282
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/vawrn4p0
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:34
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-2.0x-b
train_acc    1.0
train_loss   1.606116148119933e-05
test_acc     0.9919
test_loss    0.03482321947813034
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▄▆▆▆▇▆▇▇▇▇████▇█▇██▇██████████████████
wandb:      test_loss ██▄▃▂▂▂▃▂▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9919
wandb:      test_loss 0.03482
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run fallen-gorge-282 at: https://wandb.ai/epistoteles/partial-model-merging/runs/vawrn4p0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_200206-vawrn4p0/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_200912-sk1im5er
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-brook-284
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/sk1im5er
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:35
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-2.0x-a
train_acc    1.0
train_loss   1.578893903418551e-05
test_acc     0.9916
test_loss    0.03762076254934073
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇████████████████████
wandb:      test_loss █▅▄▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9916
wandb:      test_loss 0.03762
wandb: train_accuracy 1.0
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run youthful-brook-284 at: https://wandb.ai/epistoteles/partial-model-merging/runs/sk1im5er
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_200912-sk1im5er/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=2.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_201616-rk4di0lw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-hill-286
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/rk4di0lw
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:38
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-2.0x-b
train_acc    1.0
train_loss   2.465560057392698e-05
test_acc     0.9916
test_loss    0.04055382944643497
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▅▆▇▇▇▇█▇▇█████████████████████████████
wandb:      test_loss █▄▄▃▁▂▁▁▁▂▁▁▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9916
wandb:      test_loss 0.04055
wandb: train_accuracy 1.0
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run comic-hill-286 at: https://wandb.ai/epistoteles/partial-model-merging/runs/rk4di0lw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_201616-rk4di0lw/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=2.0, epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_202324-p8fsqsas
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-frog-287
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/p8fsqsas
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:37
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-2.0x-a
train_acc    1.0
train_loss   1.8474291865307655e-05
test_acc     0.992
test_loss    0.03802498411387205
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▆▇▇▇▇██████▇██▇█████████████████████
wandb:      test_loss █▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.03802
wandb: train_accuracy 0.99997
wandb:     train_loss 0.00011
wandb: 
wandb: 🚀 View run copper-frog-287 at: https://wandb.ai/epistoteles/partial-model-merging/runs/p8fsqsas
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_202324-p8fsqsas/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=2.0, epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_203031-4bmzjh8a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-energy-289
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/4bmzjh8a
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:35
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-2.0x-b
train_acc    1.0
train_loss   2.0877549343367718e-05
test_acc     0.992
test_loss    0.03896957505494356
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▅▆▆▇▇▇▇▇█▇▇▇▇▇█████████████████████████
wandb:      test_loss █▄▃▃▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.03897
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run northern-energy-289 at: https://wandb.ai/epistoteles/partial-model-merging/runs/4bmzjh8a
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_203031-4bmzjh8a/logs
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_203747-ly5lo5jh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-star-291
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ly5lo5jh
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:29
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-4.0x-a
train_acc    1.0
train_loss   0.00010728368930964885
test_acc     0.9915
test_loss    0.026607243157923222
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▆▅▇▇▇▇█████▇██████████████████████████
wandb:      test_loss ██▃▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.02661
wandb: train_accuracy 1.0
wandb:     train_loss 0.00028
wandb: 
wandb: 🚀 View run dainty-star-291 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ly5lo5jh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_203747-ly5lo5jh/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 260, in check_network_status
    self._loop_check_status(
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 216, in _loop_check_status
    local_handle = request()
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_204445-2uiyi1st
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-bee-293
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/2uiyi1st
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:31
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-4.0x-b
train_acc    1.0
train_loss   0.00010561905479941439
test_acc     0.9917
test_loss    0.02922386843711138
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▆▇▇▇▇▇▇▇█▇██▇████████████████████████
wandb:      test_loss █▄▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9917
wandb:      test_loss 0.02922
wandb: train_accuracy 1.0
wandb:     train_loss 0.00024
wandb: 
wandb: 🚀 View run dulcet-bee-293 at: https://wandb.ai/epistoteles/partial-model-merging/runs/2uiyi1st
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_204445-2uiyi1st/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_205148-ob4pfvwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-flower-295
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ob4pfvwp
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:28
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-4.0x-a
train_acc    1.0
train_loss   3.751736448975862e-05
test_acc     0.9928
test_loss    0.029307454265654086
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▅▄▆▆▆▇▇▇▇▇▇▇▇▇▇███▇███████████████████
wandb:      test_loss █▆▃▄▂▃▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.02931
wandb: train_accuracy 1.0
wandb:     train_loss 0.00013
wandb: 
wandb: 🚀 View run upbeat-flower-295 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ob4pfvwp
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_205148-ob4pfvwp/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_205848-rjnhd9rr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-lion-297
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/rjnhd9rr
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:34
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-4.0x-b
train_acc    1.0
train_loss   4.178028493697639e-05
test_acc     0.9928
test_loss    0.028405906818807124
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▆▆▇▇▇▇▇█▇▇▇██████████████████████████
wandb:      test_loss █▇▄▃▃▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.02841
wandb: train_accuracy 1.0
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run still-lion-297 at: https://wandb.ai/epistoteles/partial-model-merging/runs/rjnhd9rr
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_205848-rjnhd9rr/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_210552-bwtsy61b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-violet-298
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/bwtsy61b
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:34
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-4.0x-a
train_acc    1.0
train_loss   2.3172811499231708e-05
test_acc     0.9928
test_loss    0.030204611271619795
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▄▆▆▇▆▆▇▇▇▇█▇████▇█████████████████████
wandb:      test_loss ██▄▃▂▂▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.0302
wandb: train_accuracy 1.0
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run usual-violet-298 at: https://wandb.ai/epistoteles/partial-model-merging/runs/bwtsy61b
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_210552-bwtsy61b/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_211257-k5o64oy2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-snowball-300
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/k5o64oy2
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:31
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-4.0x-b
train_acc    1.0
train_loss   2.2581441983978343e-05
test_acc     0.9928
test_loss    0.03259281497448683
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▂▁▅▆▇▇▇▇▇▇▇█▇██▇████████████████████████
wandb:      test_loss ▇█▄▃▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.03259
wandb: train_accuracy 1.0
wandb:     train_loss 9e-05
wandb: 
wandb: 🚀 View run misty-snowball-300 at: https://wandb.ai/epistoteles/partial-model-merging/runs/k5o64oy2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_211257-k5o64oy2/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_212005-v8jhb1l6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-wood-301
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/v8jhb1l6
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:37
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-4.0x-a
train_acc    1.0
train_loss   1.510950191535206e-05
test_acc     0.992
test_loss    0.03709492525085807
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▄▄▆▇▆▇▇▇▇▇▇▇▇█████████████████████████
wandb:      test_loss █▇▆▅▃▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.03709
wandb: train_accuracy 1.0
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run stilted-wood-301 at: https://wandb.ai/epistoteles/partial-model-merging/runs/v8jhb1l6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_212005-v8jhb1l6/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_212712-wloqiutt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-vortex-303
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/wloqiutt
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:39
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-4.0x-b
train_acc    1.0
train_loss   1.574685013186657e-05
test_acc     0.992
test_loss    0.0354463592171669
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▄▅▆▆▇▇▇▇▇█▇▇▇█████████████████████████
wandb:      test_loss ▇█▅▄▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.03545
wandb: train_accuracy 1.0
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run zany-vortex-303 at: https://wandb.ai/epistoteles/partial-model-merging/runs/wloqiutt
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_212712-wloqiutt/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_213427-2zfrt2i1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-night-304
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/2zfrt2i1
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:40
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-4.0x-a
train_acc    1.0
train_loss   1.2744697058527283e-05
test_acc     0.9922
test_loss    0.0348925200290978
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▄▅▆▇▆▇▇▇▇▇▇▇████▇█▇███████████████████
wandb:      test_loss ██▅▃▄▂▂▂▂▂▁▂▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9922
wandb:      test_loss 0.03489
wandb: train_accuracy 1.0
wandb:     train_loss 5e-05
wandb: 
wandb: 🚀 View run light-night-304 at: https://wandb.ai/epistoteles/partial-model-merging/runs/2zfrt2i1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_213427-2zfrt2i1/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_214138-dkq7npne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-thunder-306
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/dkq7npne
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:41
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-4.0x-b
train_acc    1.0
train_loss   1.1625757823215584e-05
test_acc     0.9928
test_loss    0.03608925705775619
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▄▆▆▆▇▇▇▇▇▇▇▇▇▇██▇█████████████████████
wandb:      test_loss ▇█▅▃▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.03609
wandb: train_accuracy 1.0
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run colorful-thunder-306 at: https://wandb.ai/epistoteles/partial-model-merging/runs/dkq7npne
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_214138-dkq7npne/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_214849-wnwlthpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-fog-307
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/wnwlthpd
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:44
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-4.0x-a
train_acc    1.0
train_loss   1.1473603888134675e-05
test_acc     0.9923
test_loss    0.03851728066802025
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇████████████████████
wandb:      test_loss █▆▄▂▂▂▁▁▁▂▁▁▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9923
wandb:      test_loss 0.03852
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run olive-fog-307 at: https://wandb.ai/epistoteles/partial-model-merging/runs/wnwlthpd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_214849-wnwlthpd/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_215606-8auvdhmk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-dawn-309
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/8auvdhmk
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:48
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-4.0x-b
train_acc    1.0
train_loss   1.0807097002422476e-05
test_acc     0.9919
test_loss    0.04079369939863682
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▆▆▇▇▇▇▇▇▇███▇██▇▇█████████████████████
wandb:      test_loss █▅▃▃▂▂▁▂▁▂▁▁▁▁▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9919
wandb:      test_loss 0.04079
wandb: train_accuracy 0.99998
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run dry-dawn-309 at: https://wandb.ai/epistoteles/partial-model-merging/runs/8auvdhmk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_215606-8auvdhmk/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_220327-d2rjn7hs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-lake-310
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/d2rjn7hs
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:50
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-4.0x-a
train_acc    1.0
train_loss   1.1584813790932458e-05
test_acc     0.9927
test_loss    0.03760533360764384
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)wandb: | 0.003 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▅▇▇▆▇▇▇█▇█▇▇█▇████████████████████████
wandb:      test_loss █▆▄▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9927
wandb:      test_loss 0.03761
wandb: train_accuracy 0.99998
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run resilient-lake-310 at: https://wandb.ai/epistoteles/partial-model-merging/runs/d2rjn7hs
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_220327-d2rjn7hs/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=4.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_221046-phjf0rn4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-forest-311
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/phjf0rn4
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:43
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-4.0x-b
train_acc    1.0
train_loss   1.1032024192066577e-05
test_acc     0.992
test_loss    0.040307657420635225
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▄▆▇▇▇▇▇▇▇███▇▇████████████████████████
wandb:      test_loss █▇▆▄▂▂▂▁▂▂▂▁▁▁▂▂▁▁▂▂▂▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇█████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.992
wandb:      test_loss 0.04031
wandb: train_accuracy 1.0
wandb:     train_loss 5e-05
wandb: 
wandb: 🚀 View run frosty-forest-311 at: https://wandb.ai/epistoteles/partial-model-merging/runs/phjf0rn4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_221046-phjf0rn4/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=4.0, epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_221801-uxno0npe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sponge-313
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/uxno0npe
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:49
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-4.0x-a
train_acc    1.0
train_loss   1.3532479277955644e-05
test_acc     0.9918
test_loss    0.04229861907660961
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▃▆▆▇▇▇▇▇█▇▇▇▇█▇▇█▇████████████████████
wandb:      test_loss █▇▅▂▂▂▂▁▁▁▁▂▁▁▂▁▂▂▂▂▁▁▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9918
wandb:      test_loss 0.0423
wandb: train_accuracy 1.0
wandb:     train_loss 4e-05
wandb: 
wandb: 🚀 View run treasured-sponge-313 at: https://wandb.ai/epistoteles/partial-model-merging/runs/uxno0npe
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_221801-uxno0npe/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=4.0, epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_222526-p4ggrfd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-firebrand-314
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/p4ggrfd1
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:50
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-4.0x-b
train_acc    1.0
train_loss   1.0968400359464188e-05
test_acc     0.9925
test_loss    0.03819518107920885
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▃▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇██▇███████████████████
wandb:      test_loss ▇█▅▃▃▂▂▂▂▂▂▁▂▁▁▁▂▂▂▁▂▁▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9925
wandb:      test_loss 0.0382
wandb: train_accuracy 0.99997
wandb:     train_loss 9e-05
wandb: 
wandb: 🚀 View run youthful-firebrand-314 at: https://wandb.ai/epistoteles/partial-model-merging/runs/p4ggrfd1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_222526-p4ggrfd1/logs
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_223248-iangqgj2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-water-316
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/iangqgj2
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:38
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-8.0x-a
train_acc    1.0
train_loss   8.781427429009152e-05
test_acc     0.9923
test_loss    0.029036018438637255
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▃▁▃▆▆▆▇▇▇▇▇▇▇███████████████████████████
wandb:      test_loss ▇█▆▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9923
wandb:      test_loss 0.02904
wandb: train_accuracy 1.0
wandb:     train_loss 0.00022
wandb: 
wandb: 🚀 View run swept-water-316 at: https://wandb.ai/epistoteles/partial-model-merging/runs/iangqgj2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_223248-iangqgj2/logs
Namespace(dataset='MNIST', model_type='MLP', size=3, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_223955-y5dj0mc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-jazz-317
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/y5dj0mc3
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:32
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP3-bn-8.0x-b
train_acc    1.0
train_loss   7.953110986515336e-05
test_acc     0.9915
test_loss    0.02908136025071144
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▂▄▆▇▆▇▇▇▇█████████████████████████████
wandb:      test_loss █▅▇▅▃▂▃▃▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.02908
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00023
wandb: 
wandb: 🚀 View run bright-jazz-317 at: https://wandb.ai/epistoteles/partial-model-merging/runs/y5dj0mc3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_223955-y5dj0mc3/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_224657-dhfkaxoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-moon-318
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/dhfkaxoa
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:46
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-8.0x-a
train_acc    1.0
train_loss   3.369468670371134e-05
test_acc     0.9928
test_loss    0.030252667516469954
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: / 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▃▁▅▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████████
wandb:      test_loss ▅█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.03025
wandb: train_accuracy 1.0
wandb:     train_loss 0.0001
wandb: 
wandb: 🚀 View run silvery-moon-318 at: https://wandb.ai/epistoteles/partial-model-merging/runs/dhfkaxoa
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_224657-dhfkaxoa/logs
Namespace(dataset='MNIST', model_type='MLP', size=4, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_225411-ae4aemb2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-darkness-320
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ae4aemb2
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:41
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP4-bn-8.0x-b
train_acc    1.0
train_loss   3.692746639292939e-05
test_acc     0.993
test_loss    0.030742205679416656
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: | 0.007 MB of 0.007 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▄▁▅▅▆▆▇▇▇▇▇▇▇▇██████████████████████████
wandb:      test_loss ▅█▄▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.993
wandb:      test_loss 0.03074
wandb: train_accuracy 1.0
wandb:     train_loss 0.0001
wandb: 
wandb: 🚀 View run vibrant-darkness-320 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ae4aemb2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_225411-ae4aemb2/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_230121-6mn8yxb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-puddle-321
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/6mn8yxb3
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:58
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-8.0x-a
train_acc    1.0
train_loss   1.695697388110299e-05
test_acc     0.9928
test_loss    0.03328110910952091
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▃▃▆▆▆▇▇▇▇█▇█▇█████████████████████████
wandb:      test_loss ██▆▆▃▃▃▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9928
wandb:      test_loss 0.03328
wandb: train_accuracy 1.0
wandb:     train_loss 8e-05
wandb: 
wandb: 🚀 View run wandering-puddle-321 at: https://wandb.ai/epistoteles/partial-model-merging/runs/6mn8yxb3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_230121-6mn8yxb3/logs
Namespace(dataset='MNIST', model_type='MLP', size=5, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_230848-n8hxkc8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-glade-322
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/n8hxkc8z
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:06:56
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP5-bn-8.0x-b
train_acc    1.0
train_loss   1.9135739876219304e-05
test_acc     0.9924
test_loss    0.03416316471993923
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▂▅▆▇▇▇▇▇▇▇▇▇██▇███████████████████████
wandb:      test_loss ▆▇█▃▃▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9924
wandb:      test_loss 0.03416
wandb: train_accuracy 0.99998
wandb:     train_loss 0.00012
wandb: 
wandb: 🚀 View run genial-glade-322 at: https://wandb.ai/epistoteles/partial-model-merging/runs/n8hxkc8z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_230848-n8hxkc8z/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_231619-kks5u8m2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-jazz-323
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/kks5u8m2
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:09
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-8.0x-a
train_acc    1.0
train_loss   1.3713059629102038e-05
test_acc     0.9919
test_loss    0.035894254315644504
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▆▆▇▇▇▇▇▇██████████████████████████████
wandb:      test_loss █▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9919
wandb:      test_loss 0.03589
wandb: train_accuracy 1.0
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run brisk-jazz-323 at: https://wandb.ai/epistoteles/partial-model-merging/runs/kks5u8m2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_231619-kks5u8m2/logs
Namespace(dataset='MNIST', model_type='MLP', size=6, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_232400-aaj4m5im
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-thunder-325
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/aaj4m5im
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:07
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP6-bn-8.0x-b
train_acc    1.0
train_loss   1.150279316182908e-05
test_acc     0.9915
test_loss    0.04072891809046268
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▄▁▅▆▇▇▇▇█▇▇▇████████████████████████████
wandb:      test_loss ▅█▅▂▁▂▁▁▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.04073
wandb: train_accuracy 1.0
wandb:     train_loss 5e-05
wandb: 
wandb: 🚀 View run leafy-thunder-325 at: https://wandb.ai/epistoteles/partial-model-merging/runs/aaj4m5im
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_232400-aaj4m5im/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_233144-ecswhm8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-aardvark-326
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ecswhm8n
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:28
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-8.0x-a
train_acc    1.0
train_loss   8.094701533423176e-06
test_acc     0.9915
test_loss    0.040964114293456075
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▆▆▇▇▇▇▇▇█▇▇▇███▇██████████████████████
wandb:      test_loss █▆▃▃▂▂▁▂▂▁▁▂▁▂▁▁▂▁▁▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.04096
wandb: train_accuracy 1.0
wandb:     train_loss 4e-05
wandb: 
wandb: 🚀 View run jumping-aardvark-326 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ecswhm8n
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_233144-ecswhm8n/logs
Namespace(dataset='MNIST', model_type='MLP', size=7, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_233944-pudws39i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-pyramid-327
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/pudws39i
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:23
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP7-bn-8.0x-b
train_acc    1.0
train_loss   1.0738449847735865e-05
test_acc     0.9927
test_loss    0.04400723036378622
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▃▄▅▆▆▇▇▇▇▇▇▇▇█▇▇▇██████████████████████
wandb:      test_loss ██▆▃▃▃▁▂▂▂▂▂▁▂▁▂▂▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9927
wandb:      test_loss 0.04401
wandb: train_accuracy 1.0
wandb:     train_loss 5e-05
wandb: 
wandb: 🚀 View run astral-pyramid-327 at: https://wandb.ai/epistoteles/partial-model-merging/runs/pudws39i
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_233944-pudws39i/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_234737-9vn673sg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-silence-329
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/9vn673sg
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:42
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-8.0x-a
train_acc    1.0
train_loss   1.0746796683254918e-05
test_acc     0.9915
test_loss    0.04235548861324787
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▆▇▇▇███▇██▇█████████████████████████
wandb:      test_loss █▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9915
wandb:      test_loss 0.04236
wandb: train_accuracy 0.99997
wandb:     train_loss 9e-05
wandb: 
wandb: 🚀 View run decent-silence-329 at: https://wandb.ai/epistoteles/partial-model-merging/runs/9vn673sg
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_234737-9vn673sg/logs
Namespace(dataset='MNIST', model_type='MLP', size=8, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231126_235551-vglbqgtk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-hill-330
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/vglbqgtk
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:07:41
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP8-bn-8.0x-b
train_acc    1.0
train_loss   9.698297488588043e-06
test_acc     0.9923
test_loss    0.04084319081157446
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▆▇▇▇▇█▇▇▇█▇▇▇████▇████████████████████
wandb:      test_loss █▅▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9923
wandb:      test_loss 0.04084
wandb: train_accuracy 1.0
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run volcanic-hill-330 at: https://wandb.ai/epistoteles/partial-model-merging/runs/vglbqgtk
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231126_235551-vglbqgtk/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231127_000405-ppxyuqsh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-wood-331
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/ppxyuqsh
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:08:03
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-8.0x-a
train_acc    1.0
train_loss   1.0381544727996091e-05
test_acc     0.9924
test_loss    0.04055551737546921
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▁▆▆▇▇▇▇▇▇█▇█▇██▇███████████████████████
wandb:      test_loss ██▃▂▁▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9924
wandb:      test_loss 0.04056
wandb: train_accuracy 1.0
wandb:     train_loss 4e-05
wandb: 
wandb: 🚀 View run tough-wood-331 at: https://wandb.ai/epistoteles/partial-model-merging/runs/ppxyuqsh
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231127_000405-ppxyuqsh/logs
Namespace(dataset='MNIST', model_type='MLP', size=9, batch_norm=True, width=8.0,
epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, test=True, 
checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231127_001241-2ypm93g7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-resonance-333
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/2ypm93g7
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:08:01
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP9-bn-8.0x-b
train_acc    1.0
train_loss   9.292148835508366e-06
test_acc     0.9921
test_loss    0.040735543332993986
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▂▄▆▇▆▇▇▇▇█▇▇▇▇▇█▇▇█████████████████████
wandb:      test_loss █▇▄▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9921
wandb:      test_loss 0.04074
wandb: train_accuracy 1.0
wandb:     train_loss 4e-05
wandb: 
wandb: 🚀 View run classic-resonance-333 at: https://wandb.ai/epistoteles/partial-model-merging/runs/2ypm93g7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231127_001241-2ypm93g7/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=8.0, epochs=100, lr=0.2, weight_decay=0.0, variant='a', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231127_002114-i7pji2df
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-waterfall-334
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/i7pji2df
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:08:24
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-8.0x-a
train_acc    1.0
train_loss   9.010327609833742e-06
test_acc     0.9924
test_loss    0.040674279630184176
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: / 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▇▇▇▇▇▇▇▇▇█▇█████████████████████████
wandb:      test_loss █▆▃▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9924
wandb:      test_loss 0.04067
wandb: train_accuracy 0.99998
wandb:     train_loss 6e-05
wandb: 
wandb: 🚀 View run rosy-waterfall-334 at: https://wandb.ai/epistoteles/partial-model-merging/runs/i7pji2df
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231127_002114-i7pji2df/logs
Namespace(dataset='MNIST', model_type='MLP', size=10, batch_norm=True, 
width=8.0, epochs=100, lr=0.2, weight_decay=0.0, variant='b', wandb=True, 
test=True, checkpoint_midway=False)
wandb: Currently logged in as: epistoteles. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/korbinian/Documents/partial-model-merging/wandb/run-20231127_003015-v5yrblxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-serenity-335
wandb: ⭐️ View project at https://wandb.ai/epistoteles/partial-model-merging
wandb: 🚀 View run at https://wandb.ai/epistoteles/partial-model-merging/runs/v5yrblxe
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:08:26
/home/korbinian/Documents/partial-model-merging/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  input = module(input)
📥 Accuracies and losses saved for MNIST-MLP10-bn-8.0x-b
train_acc    1.0
train_loss   7.582042668256387e-06
test_acc     0.9926
test_loss    0.0392862131819129
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  learning_rate ▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:  test_accuracy ▁▄▅▆▆▇▇▇▇▇▇▇█▇██████████████████████████
wandb:      test_loss █▅▄▃▂▁▁▂▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_accuracy ▁▇▇▇████████████████████████████████████
wandb:     train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          epoch 99
wandb:  learning_rate 0.0
wandb:  test_accuracy 0.9926
wandb:      test_loss 0.03929
wandb: train_accuracy 0.99997
wandb:     train_loss 7e-05
wandb: 
wandb: 🚀 View run neat-serenity-335 at: https://wandb.ai/epistoteles/partial-model-merging/runs/v5yrblxe
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231127_003015-v5yrblxe/logs
